# Human-in-the-Loop (HITL) Training System

## Overview

Backlog Chef learns from **your team's domain expertise** to improve PBI quality scoring over time. This document describes the training system that collects feedback at each pipeline step and uses it to refine the AI's behavior.

## Why HITL Training is Critical

### The Challenge
- **PBI quality is subjective** - What's "complete" varies by team, domain, tech stack
- **Domain-specific patterns** - Salesforce teams have different needs than web app teams
- **Organization-specific standards** - Your Definition of Ready may differ from others
- **Context is king** - "Similar work" depends on your historical backlog, not generic patterns

### The Solution
- Capture corrections and guidance from Product Owners, Developers, and Scrum Masters
- Build a **team-specific knowledge base** of quality patterns
- Continuously improve confidence scoring based on your feedback
- Learn where to find relevant context for your organization

---

## Training Architecture

### Three-Layer Learning System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: IMMEDIATE FEEDBACK (Real-time Corrections)         â”‚
â”‚  - Correct confidence scores                                 â”‚
â”‚  - Identify missing context sources                          â”‚
â”‚  - Flag incorrect risk detections                            â”‚
â”‚  - Override readiness classifications                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: TRAINING DATA COLLECTION (Structured Feedback)     â”‚
â”‚  - Store feedback with original PBI + output                 â”‚
â”‚  - Tag feedback by pipeline step                             â”‚
â”‚  - Track correction patterns over time                       â”‚
â”‚  - Build golden dataset for fine-tuning                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: ADAPTIVE LEARNING (System Improvements)            â”‚
â”‚  - Few-shot prompt examples from feedback                    â”‚
â”‚  - Custom scoring rubrics per team                           â”‚
â”‚  - Learned context source priorities                         â”‚
â”‚  - Fine-tuned models (future)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Feedback Mechanisms by Pipeline Step

### Step 1: Event Detection
**What can go wrong**: Misclassifies meeting type (refinement vs retrospective vs planning)

**HITL Feedback Interface**:
```typescript
interface EventDetectionFeedback {
  detected_event: 'refinement' | 'planning' | 'retrospective' | 'other';
  correct_event: 'refinement' | 'planning' | 'retrospective' | 'other';
  confidence_was: number;
  correction_reason: string;
  indicators_missed?: string[];  // "Team discussed story points"
}
```

**Example UI**:
```
âŒ Detected: Planning Meeting (confidence: 75%)

Correct event type:
â—‹ Refinement Meeting
â—‹ Sprint Planning
â—‹ Retrospective
â— Other: Architecture Discussion

Why was detection wrong?
"Meeting discussed technical approach, not sprint commitment"

Indicators the system missed:
- "No velocity or capacity discussion"
- "Focused on design patterns, not tasks"

[Submit Correction]
```

**How system learns**:
- Stores corrected examples in `training-data/event-detection/corrections.jsonl`
- Adds correction as few-shot example in next prompt
- After 10+ corrections of same type, updates base prompt

---

### Step 2: Extract Candidate PBIs
**What can go wrong**: Misses a PBI mentioned in meeting, or creates duplicate PBIs

**HITL Feedback Interface**:
```typescript
interface ExtractionFeedback {
  extracted_pbis: string[];  // IDs of extracted PBIs
  corrections: {
    type: 'missed_pbi' | 'duplicate' | 'incorrectly_split' | 'incorrectly_merged';
    description: string;
    transcript_reference: string;  // Timestamp or quote
    correct_pbi?: {
      title: string;
      description: string;
    };
  }[];
}
```

**Example UI**:
```
Extracted 3 PBIs from transcript:

âœ“ PBI-001: Customer Order Tracking Portal
âœ“ PBI-002: Customer-Friendly Order Status Labels
âœ“ PBI-003: B2B Account Permission Model

Did the system miss any PBIs?
[+ Add Missed PBI]

Was anything incorrectly extracted?
PBI-003 should be merged with PBI-001
Reason: "Team discussed permissions as part of portal feature, not separate PBI"
Transcript quote: "Sarah: The portal needs role-based permissions..."

[Submit Correction]
```

**How system learns**:
- Stores transcript excerpts where PBIs were missed
- Learns patterns like "Later, Tom mentioned..." = separate PBI
- Adjusts PBI boundary detection (when to split vs merge)

---

### Step 3: Score Confidence
**What can go wrong**: Over/underestimates readiness scores, misses critical gaps

**HITL Feedback Interface**:
```typescript
interface ConfidenceScoringFeedback {
  pbi_id: string;
  score_corrections: {
    dimension: 'isCompletePBI' | 'hasAllRequirements' | 'isRefinementComplete' |
               'hasAcceptanceCriteria' | 'hasClearScope' | 'isEstimable';
    system_score: number;
    correct_score: number;
    reasoning_was: string;
    correct_reasoning: string;
    evidence_missed?: string[];
  }[];
  overall_readiness_correction?: {
    system_readiness: 'READY' | 'MOSTLY_READY' | 'NOT_READY' | 'DEFERRED';
    correct_readiness: 'READY' | 'MOSTLY_READY' | 'NOT_READY' | 'DEFERRED';
    reason: string;
  };
}
```

**Example UI** (Interactive Score Review):
```
PBI-001: Customer Order Tracking Portal

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ isCompletePBI                              [Edit Score] â”‚
â”‚ System Score: 90/100                                    â”‚
â”‚ Reasoning: "Clear business value, specific user need"  â”‚
â”‚                                                         â”‚
â”‚ âœï¸ Your Score: 75/100                                   â”‚
â”‚ Reason: "Missing specific ROI metrics for business"    â”‚
â”‚ Evidence Missed:                                        â”‚
â”‚ - "No customer support time savings quantified"        â”‚
â”‚ - "Missing user adoption targets"                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ hasAllRequirements                          [Edit Score]â”‚
â”‚ System Score: 85/100                                    â”‚
â”‚ Reasoning: "Comprehensive technical requirements"      â”‚
â”‚                                                         â”‚
â”‚ âœï¸ Your Score: 60/100                                   â”‚
â”‚ Reason: "License capacity unknown - CRITICAL blocker"  â”‚
â”‚ Evidence Missed:                                        â”‚
â”‚ - "No confirmation of 1200 Experience Cloud licenses"  â”‚
â”‚ - "Cost approval not obtained"                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Overall Readiness:
System: READY âŒ
Correct: NOT_READY (blocking: license capacity)

[Save All Corrections]
```

**How system learns**:
- Builds team-specific scoring rubric (e.g., "license capacity always checked")
- Learns which evidence is critical vs nice-to-have
- Adjusts score thresholds (maybe your team's "READY" = 85+, not 70+)

---

### Step 4: Enrich with Context
**What can go wrong**: Misses relevant past work, searches wrong locations, misidentifies "similar"

**HITL Feedback Interface**:
```typescript
interface ContextEnrichmentFeedback {
  pbi_id: string;
  found_context: {
    type: 'similar_work' | 'past_decisions' | 'technical_docs';
    ref: string;
    relevance_score: number;
  }[];
  corrections: {
    type: 'missed_context' | 'irrelevant_context' | 'wrong_source';
    description: string;
    action: {
      type: 'add_context' | 'remove_context' | 'adjust_query' | 'add_source';
      details: {
        // Add context
        context_location?: string;  // "Confluence: https://..."
        why_relevant?: string;

        // Remove context
        context_id?: string;
        why_irrelevant?: string;

        // Adjust query
        better_query?: string;

        // Add source
        new_source?: {
          type: 'confluence_space' | 'devops_query' | 'sharepoint_folder';
          location: string;
        };
      };
    };
  }[];
}
```

**Example UI** (Guided Feedback):
```
PBI-001: Customer Order Tracking Portal

Context Found (5 results):
âœ“ PBI-2023-156: Partner Portal Implementation (78% similar)
âœ“ CONF-Portal-Architecture (65% similar)
? PBI-2022-034: Mobile App Redesign (42% similar)  [Mark Irrelevant]
âœ“ Meeting-2024-10-15: Q4 Architecture Review
? CONF-API-Limits (38% similar)  [Mark Irrelevant]

Did the system miss important context?
[+ Add Missing Context]

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Add Missing Context                                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Context Type:                                          â•‘
â•‘ â— Similar Work                                         â•‘
â•‘ â—‹ Past Decision                                        â•‘
â•‘ â—‹ Technical Documentation                              â•‘
â•‘                                                        â•‘
â•‘ Where is it located?                                   â•‘
â•‘ â—‹ Azure DevOps (work item ID): PBI-2024-201           â•‘
â•‘ â— Confluence (page URL):                               â•‘
â•‘   https://confluence.company.com/experience-cloud-     â•‘
â•‘   performance-guide                                    â•‘
â•‘ â—‹ Fireflies (meeting date): ___________               â•‘
â•‘ â—‹ Other: _____________________________________         â•‘
â•‘                                                        â•‘
â•‘ Why is this relevant?                                  â•‘
â•‘ "This guide documents caching strategies we used for  â•‘
â•‘  the Partner Portal that had the same performance      â•‘
â•‘  requirements. System should have found this."         â•‘
â•‘                                                        â•‘
â•‘ Search query that should have found it:                â•‘
â•‘ "Experience Cloud performance optimization caching"    â•‘
â•‘                                                        â•‘
â•‘ [Cancel]  [Add to Knowledge Base]                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[Submit All Feedback]
```

**How system learns**:
- Stores successful context locations per PBI pattern
- Learns better search queries for your domain
- Discovers new context sources (e.g., "always check #architecture Slack channel")
- Builds MCP query templates: "For portal PBIs, search Confluence space TECH with keywords [portal, performance, caching]"

---

### Step 5: Check Risks & Conflicts
**What can go wrong**: Misses critical risks, flags false positives, underestimates complexity

**HITL Feedback Interface**:
```typescript
interface RiskCheckFeedback {
  pbi_id: string;
  detected_risks: {
    type: string;
    severity: 'CRITICAL' | 'HIGH' | 'MEDIUM' | 'LOW';
    description: string;
  }[];
  corrections: {
    type: 'missed_risk' | 'false_positive' | 'wrong_severity';
    details: {
      // Missed risk
      risk_type?: string;
      risk_severity?: 'CRITICAL' | 'HIGH' | 'MEDIUM' | 'LOW';
      risk_description?: string;
      why_missed?: string;

      // False positive
      flagged_risk_id?: string;
      why_not_a_risk?: string;

      // Wrong severity
      risk_id?: string;
      correct_severity?: 'CRITICAL' | 'HIGH' | 'MEDIUM' | 'LOW';
      severity_reasoning?: string;
    };
  }[];
  complexity_score_correction?: {
    system_score: number;  // 0-10
    correct_score: number;
    reasoning: string;
  };
}
```

**Example UI**:
```
PBI-001: Customer Order Tracking Portal

Detected Risks:
ğŸ”´ CRITICAL: License capacity insufficient
   âœ“ Confirmed - this is a real blocker

ğŸŸ¡ MEDIUM: Performance risk with large datasets
   âŒ Should be HIGH - we've had production incidents before
   [Adjust Severity]

Missed Risks (add below):
[+ Add Missed Risk]

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Add Missed Risk                                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Risk Type:                                             â•‘
â•‘ â— Security/Compliance                                  â•‘
â•‘ â—‹ Technical Complexity                                 â•‘
â•‘ â—‹ Resource Constraint                                  â•‘
â•‘ â—‹ Dependency                                           â•‘
â•‘                                                        â•‘
â•‘ Severity: â— CRITICAL  â—‹ HIGH  â—‹ MEDIUM  â—‹ LOW         â•‘
â•‘                                                        â•‘
â•‘ Description:                                           â•‘
â•‘ "Customer PII exposure risk - portal shows order data  â•‘
â•‘  but we haven't defined data retention policy for      â•‘
â•‘  external users. GDPR violation risk."                 â•‘
â•‘                                                        â•‘
â•‘ Why did system miss this?                              â•‘
â•‘ "System should check for PII/GDPR whenever 'external   â•‘
â•‘  user access' is mentioned"                            â•‘
â•‘                                                        â•‘
â•‘ Where should system have found this?                   â•‘
â•‘ â˜‘ Past similar work (PBI-2023-045 had same issue)     â•‘
â•‘ â˜‘ Confluence: GDPR Compliance Checklist                â•‘
â•‘ â˜ Meeting transcript                                   â•‘
â•‘                                                        â•‘
â•‘ [Cancel]  [Add Risk]                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Complexity Score:
System: 8.5/10  â†’  Your Score: 9.5/10
Reason: "Experience Cloud licenses are hard to get approved quickly,
         adds 2-3 week delay minimum"

[Submit Feedback]
```

**How system learns**:
- Builds team-specific risk patterns (e.g., "always check GDPR for external users")
- Learns severity calibration (what's CRITICAL for your team vs just MEDIUM)
- Discovers risk indicators from past incidents
- Creates risk detection rules: "IF (external_users AND personal_data) THEN check_gdpr_compliance"

---

### Step 6: Generate Questions & Proposals
**What can go wrong**: Generates irrelevant questions, misses critical unknowns, routes to wrong stakeholder

**HITL Feedback Interface**:
```typescript
interface QuestionGenerationFeedback {
  pbi_id: string;
  generated_questions: {
    id: string;
    priority: 'CRITICAL' | 'HIGH' | 'MEDIUM' | 'LOW';
    question: string;
    proposed_answer: string;
    assigned_to: string;  // Role
  }[];
  corrections: {
    question_id?: string;
    type: 'irrelevant' | 'wrong_priority' | 'wrong_stakeholder' | 'missed_question';
    details: {
      // Irrelevant
      why_irrelevant?: string;

      // Wrong priority
      correct_priority?: 'CRITICAL' | 'HIGH' | 'MEDIUM' | 'LOW';
      priority_reasoning?: string;

      // Wrong stakeholder
      correct_stakeholder?: string;
      routing_reasoning?: string;

      // Missed question
      new_question?: {
        priority: 'CRITICAL' | 'HIGH' | 'MEDIUM' | 'LOW';
        question: string;
        why_critical?: string;
        assign_to: string;
      };
    };
  }[];
}
```

**Example UI**:
```
PBI-001: Customer Order Tracking Portal

Generated Questions (12):

ğŸ”´ CRITICAL: "Do we have budget approval for 700 additional licenses?"
   Assigned to: Sarah (Product Owner)
   âœ“ Confirmed - this is critical

ğŸŸ¡ HIGH: "What caching strategy should we use?"
   Assigned to: Lisa (Developer)
   âŒ Wrong stakeholder - should be "Tech Lead/Architect"
   [Reassign]

ğŸŸ¢ MEDIUM: "Should we support IE11?"
   âŒ Irrelevant - IE11 is deprecated, remove this
   [Mark Irrelevant]

Missed Questions:
[+ Add Critical Question]

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Add Missed Question                                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Priority: â— CRITICAL  â—‹ HIGH  â—‹ MEDIUM  â—‹ LOW         â•‘
â•‘                                                        â•‘
â•‘ Question:                                              â•‘
â•‘ "Have we verified that Experience Cloud supports our   â•‘
â•‘  custom Order object with 150+ fields?"                â•‘
â•‘                                                        â•‘
â•‘ Why is this critical?                                  â•‘
â•‘ "Similar project (Partner Portal) hit Governor limits  â•‘
â•‘  with complex objects. This is a blocker."             â•‘
â•‘                                                        â•‘
â•‘ Assign to:                                             â•‘
â•‘ â— Tech Lead/Architect                                  â•‘
â•‘ â—‹ Product Owner                                        â•‘
â•‘ â—‹ Developer                                            â•‘
â•‘                                                        â•‘
â•‘ Suggested answer (optional):                           â•‘
â•‘ "Check with Salesforce support, reference case #12345  â•‘
â•‘  from Partner Portal project"                          â•‘
â•‘                                                        â•‘
â•‘ [Cancel]  [Add Question]                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[Submit Feedback]
```

**How system learns**:
- Learns team-specific critical questions (always ask about licenses, performance, GDPR)
- Improves stakeholder routing (Tech Lead for architecture, PO for business decisions)
- Builds question templates per PBI type (portal PBIs â†’ ask about performance, licenses, security)

---

### Step 7: Run Readiness Checker
**What can go wrong**: Incorrectly classifies as READY when blockers exist, or too conservative

**HITL Feedback Interface**:
```typescript
interface ReadinessCheckFeedback {
  pbi_id: string;
  system_readiness: 'READY_FOR_SPRINT' | 'NOT_READY' | 'NEEDS_REFINEMENT' |
                    'DEFERRED' | 'FUTURE_PHASE';
  correct_readiness: 'READY_FOR_SPRINT' | 'NOT_READY' | 'NEEDS_REFINEMENT' |
                     'DEFERRED' | 'FUTURE_PHASE';
  reasoning: string;
  checklist_overrides: {
    criterion: string;
    system_result: 'PASS' | 'FAIL' | 'WARN';
    correct_result: 'PASS' | 'FAIL' | 'WARN';
    reasoning: string;
  }[];
}
```

**Example UI**:
```
PBI-001: Customer Order Tracking Portal

System Classification: READY FOR SPRINT âŒ
Your Classification: NOT READY

Definition of Ready Checklist:

âœ“ Clear business value          [PASS]
âœ“ Acceptance criteria defined   [PASS]
âŒ Dependencies resolved         [FAIL - license approval pending]
âœ“ Estimated by team             [PASS]
âš ï¸  Technical approach agreed    [WARN]
   System: WARN â†’ Should be: FAIL
   Reason: "No performance design = can't start development"

âš ï¸  No blocking unknowns         [WARN]
   System: WARN â†’ Should be: FAIL
   Reason: "GDPR data retention policy is a blocker"

Why is this NOT READY?
"Two critical blockers:
 1. License budget approval required (4-6 week process)
 2. GDPR data retention policy undefined (legal review needed)

 Cannot start sprint until both resolved."

[Submit Classification]
```

**How system learns**:
- Calibrates team-specific Definition of Ready
- Learns which criteria are "nice-to-have" vs "must-have"
- Adjusts readiness thresholds (your team may be stricter/looser than default)

---

### Step 8: Final Output
**What can go wrong**: Formatting issues, missing required fields, wrong destination format

**HITL Feedback Interface**:
```typescript
interface OutputFormatFeedback {
  pbi_id: string;
  destination: 'devops' | 'obsidian' | 'confluence';
  issues: {
    type: 'missing_field' | 'incorrect_format' | 'wrong_mapping';
    field_name: string;
    expected_value: string;
    actual_value: string;
    fix_description: string;
  }[];
}
```

**Example UI**:
```
PBI-001 â†’ Azure DevOps

Preview:
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Title: Customer Self-Service Order Portal             â•‘
â•‘ Description: Enable B2B and B2C customers to...        â•‘
â•‘ Acceptance Criteria: [6 criteria listed]              â•‘
â•‘ Tags: portal, experience-cloud, b2b                    â•‘
â•‘ Priority: High                                         â•‘
â•‘ Effort: Not estimated                                  â•‘
â•‘ Status: NOT READY                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Issues:
âŒ Missing field: "Area Path" (required by our DevOps setup)
   Should be: "CustomerPortal\\Frontend"

âŒ Wrong mapping: Priority "High" â†’ DevOps "2"
   Should be: Priority "High" â†’ DevOps "1" (we use 1=High, not 2)

[Fix & Regenerate] [Submit Feedback]
```

**How system learns**:
- Stores team-specific field mappings
- Learns required vs optional fields per destination
- Discovers custom fields and their expected values

---

## Training Data Storage

### File Structure
```
/training-data/
â”œâ”€â”€ event-detection/
â”‚   â”œâ”€â”€ corrections.jsonl          # Corrected event classifications
â”‚   â””â”€â”€ few-shot-examples.json     # Best examples for prompts
â”œâ”€â”€ extraction/
â”‚   â”œâ”€â”€ missed-pbis.jsonl          # PBIs system failed to extract
â”‚   â”œâ”€â”€ incorrect-splits.jsonl     # PBIs wrongly split/merged
â”‚   â””â”€â”€ few-shot-examples.json
â”œâ”€â”€ scoring/
â”‚   â”œâ”€â”€ score-corrections.jsonl    # Corrected confidence scores
â”‚   â”œâ”€â”€ team-rubric.yaml           # Team-specific scoring criteria
â”‚   â””â”€â”€ few-shot-examples.json
â”œâ”€â”€ enrichment/
â”‚   â”œâ”€â”€ context-corrections.jsonl  # Missed/irrelevant context
â”‚   â”œâ”€â”€ search-queries.jsonl       # Better search queries
â”‚   â”œâ”€â”€ context-sources.yaml       # Learned context locations
â”‚   â””â”€â”€ few-shot-examples.json
â”œâ”€â”€ risk-detection/
â”‚   â”œâ”€â”€ missed-risks.jsonl
â”‚   â”œâ”€â”€ false-positives.jsonl
â”‚   â”œâ”€â”€ risk-patterns.yaml         # Team-specific risk rules
â”‚   â””â”€â”€ few-shot-examples.json
â”œâ”€â”€ questions/
â”‚   â”œâ”€â”€ question-corrections.jsonl
â”‚   â”œâ”€â”€ stakeholder-routing.yaml   # Role assignment rules
â”‚   â””â”€â”€ few-shot-examples.json
â”œâ”€â”€ readiness/
â”‚   â”œâ”€â”€ classification-corrections.jsonl
â”‚   â”œâ”€â”€ definition-of-ready.yaml   # Team's DoR criteria
â”‚   â””â”€â”€ few-shot-examples.json
â””â”€â”€ output/
    â”œâ”€â”€ format-corrections.jsonl
    â””â”€â”€ field-mappings.yaml        # DevOps/Confluence field mappings
```

### Example Training Data Record

**File**: `training-data/scoring/score-corrections.jsonl`
```json
{
  "timestamp": "2025-11-18T15:30:00Z",
  "pbi_id": "PBI-001",
  "pbi_title": "Customer Order Tracking Portal",
  "pipeline_step": "score_confidence",
  "original_output": {
    "hasAllRequirements": {
      "score": 85,
      "reasoning": "Comprehensive technical requirements with clear integration",
      "evidence": ["Specific technical approach", "Performance optimization mentioned"]
    }
  },
  "correction": {
    "hasAllRequirements": {
      "score": 60,
      "reasoning": "License capacity unknown - CRITICAL blocker",
      "evidence": ["No confirmation of 1200 Experience Cloud licenses", "Cost approval not obtained"]
    }
  },
  "feedback_metadata": {
    "corrected_by": "sarah.jones@company.com",
    "corrected_by_role": "Product Owner",
    "correction_reason": "System missed critical business blocker",
    "learning": "Always check license capacity for Experience Cloud portals - this is ALWAYS a blocker if not confirmed"
  }
}
```

---

## Adaptive Learning Mechanisms

### Level 1: Few-Shot Prompt Engineering (Immediate)

**How it works**: After each correction, add it to the next prompt as an example

**Example**:
```typescript
// Before feedback
const prompt = `
Score this PBI for hasAllRequirements (0-100):
${JSON.stringify(pbi)}
`;

// After 3 corrections about license capacity
const prompt = `
Score this PBI for hasAllRequirements (0-100):

IMPORTANT: Based on past feedback, always check:
- License capacity for Experience Cloud (CRITICAL blocker if not confirmed)
- Cost approval status for license purchases
- GDPR compliance for external user access

Examples from this team:

Example 1:
PBI: "Partner Portal Implementation"
Score: 40/100 (not 75/100)
Reason: "License capacity not confirmed - 500 available but 1200 needed"

Example 2:
PBI: "Customer Self-Service Portal"
Score: 60/100 (not 85/100)
Reason: "Cost approval for licenses not obtained"

Now score this PBI:
${JSON.stringify(pbi)}
`;
```

**Trigger**: After 3 corrections of same pattern
**Update frequency**: Real-time

---

### Level 2: Custom Configuration Files (Weekly)

**How it works**: Generate team-specific YAML configs from accumulated feedback

**Example**: `training-data/scoring/team-rubric.yaml`
```yaml
team_id: "salesforce-platform-team"
updated_at: "2025-11-18"

scoring_adjustments:
  hasAllRequirements:
    critical_checks:
      - pattern: "portal|experience cloud"
        check: "license_capacity_confirmed"
        weight: 30  # 30 points deduction if missing
        evidence_required:
          - "License count confirmed"
          - "Budget approval obtained"

      - pattern: "external user|b2b|b2c"
        check: "gdpr_compliance_verified"
        weight: 25
        evidence_required:
          - "Data retention policy defined"
          - "Legal team sign-off"

      - pattern: "integration|api"
        check: "api_limits_verified"
        weight: 15
        evidence_required:
          - "Daily API limit checked"
          - "Bulk API usage estimated"

  isEstimable:
    complexity_factors:
      - pattern: "experience cloud"
        complexity_multiplier: 1.5
        reasoning: "Team historically underestimates portal projects by 50%"

      - pattern: "gdpr|compliance"
        complexity_multiplier: 1.3
        reasoning: "Legal reviews add 2-3 week delays"

definition_of_ready:
  required_criteria:
    - name: "License capacity confirmed"
      applies_when: "experience cloud|portal"
      severity: "BLOCKING"

    - name: "GDPR compliance verified"
      applies_when: "external user|personal data"
      severity: "BLOCKING"

    - name: "API limits checked"
      applies_when: "integration|api"
      severity: "WARNING"
```

**Trigger**: Weekly batch processing of feedback
**Update frequency**: Every Sunday night

---

### Level 3: Learned Context Sources (Weekly)

**How it works**: Build a map of where to find context for different PBI patterns

**Example**: `training-data/enrichment/context-sources.yaml`
```yaml
team_id: "salesforce-platform-team"
updated_at: "2025-11-18"

learned_sources:
  - pbi_pattern: "portal|experience cloud"
    context_sources:
      - type: "confluence"
        space: "TECH"
        search_query: "Experience Cloud performance caching"
        priority: 1
        success_rate: 85%  # Found relevant context 85% of time

      - type: "devops"
        query: "WorkItemType = 'User Story' AND Tags CONTAINS 'portal'"
        priority: 2
        success_rate: 78%

      - type: "confluence"
        page_id: "12345"
        title: "Experience Cloud Architecture Guide"
        always_include: true
        reasoning: "Always relevant for portal PBIs"

  - pbi_pattern: "gdpr|compliance|external user"
    context_sources:
      - type: "confluence"
        space: "LEGAL"
        search_query: "GDPR compliance checklist"
        priority: 1
        success_rate: 95%

      - type: "slack"
        channel: "#legal-compliance"
        search_query: "data retention policy"
        priority: 2
        success_rate: 60%
        note: "Check for past legal discussions"

  - pbi_pattern: "integration|api"
    context_sources:
      - type: "confluence"
        space: "TECH"
        search_query: "Salesforce API limits best practices"
        priority: 1

      - type: "devops"
        query: "Tags CONTAINS 'integration' AND State = 'Done'"
        priority: 2
        success_rate: 70%
```

**How to use**:
```typescript
async function findContextSources(pbi: PBI): Promise<ContextSource[]> {
  const learnedSources = loadLearnedSources('team-rubric.yaml');

  // Match PBI against patterns
  for (const pattern of learnedSources.learned_sources) {
    if (pbi.title.match(pattern.pbi_pattern) ||
        pbi.description.match(pattern.pbi_pattern)) {
      return pattern.context_sources.sort((a, b) => a.priority - b.priority);
    }
  }

  return defaultContextSources;
}
```

---

### Level 4: Fine-Tuned Models (Future - V2)

**How it works**: Once 100+ corrected examples collected, fine-tune Claude

**Datasets to collect**:
1. **Confidence Scoring**: 100+ PBIs with corrected scores
2. **Risk Detection**: 100+ PBIs with team-specific risks
3. **Question Generation**: 100+ PBIs with critical questions

**Example fine-tuning dataset**:
```jsonl
{"prompt": "Score hasAllRequirements for this Salesforce portal PBI:\n{...pbi...}", "completion": "Score: 60/100. CRITICAL: License capacity not confirmed. Evidence required: [...]"}
{"prompt": "Score hasAllRequirements for this Salesforce portal PBI:\n{...pbi...}", "completion": "Score: 40/100. BLOCKING: GDPR compliance not verified. Evidence required: [...]"}
...
```

**ROI calculation**:
- Fine-tuning cost: ~$100-500 (one-time)
- Inference cost reduction: 30-50% (smaller, faster model)
- Accuracy improvement: 15-25%

**Trigger**: After 100+ corrections per dimension
**Update frequency**: Quarterly

---

## Implementation Roadmap

### Phase 1: Basic Feedback Collection (Week 1-2)
**Goal**: Capture corrections in structured format

- [ ] Build feedback UI components for each step
- [ ] Store corrections in JSONL files
- [ ] Add "Submit Feedback" buttons to pipeline output

**Success Criteria**: Can collect and store feedback for all 8 steps

---

### Phase 2: Few-Shot Learning (Week 3-4)
**Goal**: Use feedback immediately in next run

- [ ] Implement few-shot example injection into prompts
- [ ] Build "golden examples" selector (picks best 3-5 examples)
- [ ] Add feedbackâ†’prompt pipeline

**Success Criteria**: Prompts include team-specific examples

---

### Phase 3: Configuration Generation (Week 5-6)
**Goal**: Auto-generate team-specific configs weekly

- [ ] Build YAML config generator from feedback
- [ ] Implement config-driven scoring adjustments
- [ ] Create learned context source mapper

**Success Criteria**: System uses team rubric automatically

---

### Phase 4: Active Learning UI (Week 7-8)
**Goal**: Proactively ask for feedback when uncertain

- [ ] Add confidence scores to AI outputs
- [ ] Surface "low confidence" predictions for review
- [ ] Implement "Review Queue" for borderline PBIs

**Success Criteria**: Users review only uncertain predictions

---

### Phase 5: Analytics Dashboard (Week 9-10)
**Goal**: Show improvement over time

- [ ] Build training analytics dashboard
- [ ] Track: feedback volume, accuracy trends, common correction patterns
- [ ] Generate weekly "learning report"

**Success Criteria**: Product Owner can see system improvement

---

## Feedback Loop Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WEEK 1: Initial Run (No Training Data)                     â”‚
â”‚  Confidence Scoring Accuracy: 60%                           â”‚
â”‚  User corrects 15 PBIs                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WEEK 2: Few-Shot Examples Added                            â”‚
â”‚  Confidence Scoring Accuracy: 72% (+12%)                    â”‚
â”‚  User corrects 8 PBIs (fewer corrections needed!)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WEEK 3: Team Rubric Generated                              â”‚
â”‚  Confidence Scoring Accuracy: 81% (+9%)                     â”‚
â”‚  System auto-checks license capacity, GDPR compliance       â”‚
â”‚  User corrects 3 PBIs                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WEEK 4+: Learned Context Sources Active                    â”‚
â”‚  Confidence Scoring Accuracy: 88% (+7%)                     â”‚
â”‚  Context enrichment finds relevant docs 90% of time         â”‚
â”‚  User trusts system, minimal corrections                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## CLI Commands for Training

### Collect Feedback
```bash
# Interactive feedback mode after processing
backlog-chef process --feedback-mode transcript.json

# Review specific PBI
backlog-chef review PBI-001 --step scoring

# Batch review multiple PBIs
backlog-chef review-batch --pbis PBI-001,PBI-002,PBI-003
```

### View Training Data
```bash
# Show all corrections for a step
backlog-chef training show --step scoring

# Show learning progress
backlog-chef training stats

# Export training data for analysis
backlog-chef training export --format csv --output training-stats.csv
```

### Generate Configs
```bash
# Manually trigger config generation
backlog-chef training generate-config

# Preview what configs would be generated
backlog-chef training preview-config

# Test config impact
backlog-chef training test-config --pbi PBI-001
```

---

## Metrics to Track

### Accuracy Metrics
- **Confidence Score MAE** (Mean Absolute Error): Average difference between system scores and human scores
- **Readiness Classification Accuracy**: % of PBIs correctly classified as READY/NOT_READY
- **Context Relevance Score**: % of found context marked as "relevant" by users
- **Risk Detection Recall**: % of actual risks detected by system

### Efficiency Metrics
- **Corrections per PBI**: Number of corrections needed per PBI (should decrease over time)
- **Review Time**: Time spent by users reviewing/correcting outputs
- **Feedback Adoption Rate**: % of user feedback successfully incorporated into next run

### Business Metrics
- **PBIs Entering Sprint with Blockers**: Should decrease as system improves
- **Sprint Planning Rework**: Time spent in planning clarifying unclear PBIs
- **Team Confidence in AI Output**: Survey score (1-10)

---

## Example: Complete Training Workflow

### Scenario: Training the System to Check License Capacity

**Week 1 - Initial Feedback**:
```
User processes PBI-001 (Portal project)
System scores hasAllRequirements: 85/100
User corrects to 60/100: "License capacity not confirmed"
â†’ Stored in training-data/scoring/corrections.jsonl
```

**Week 2 - Few-Shot Learning**:
```
User processes PBI-015 (Another portal project)
System prompt now includes:
  "IMPORTANT: For portal PBIs, always verify license capacity"
  Example: PBI-001 was 60/100 because licenses not confirmed
System scores hasAllRequirements: 65/100 âœ“ (learned!)
User confirms: "Correct - licenses are still uncertain"
â†’ System is learning
```

**Week 3 - Pattern Recognition**:
```
System has seen 5 corrections about license capacity
Weekly config generation runs
â†’ Creates rule in team-rubric.yaml:
  "pattern: portal â†’ check: license_capacity_confirmed â†’ weight: 30"

Next portal PBI automatically deducts 30 points if licenses not mentioned
System scores hasAllRequirements: 55/100 (auto-detected issue!)
User confirms: "Perfect - this IS a blocker"
â†’ No correction needed
```

**Week 4 - Proactive Detection**:
```
User processes PBI-025 (New portal project)
System:
  1. Detects "portal" keyword
  2. Applies license_capacity rule
  3. Searches for "license" in meeting transcript
  4. Finds: "Sarah: We'll check license availability next week"
  5. Scores: 45/100
  6. Flags: CRITICAL - License capacity not confirmed
  7. Generates question: "How many Experience Cloud licenses are available?"

User: "Exactly right! No corrections needed."
â†’ System now autonomously checks licenses for portal PBIs
```

---

## Privacy & Security

### Feedback Data
- **PII Redaction**: Automatically redact customer names, emails from training data
- **Access Control**: Only team members can see/correct their team's feedback
- **Data Retention**: Training data stored for 2 years, then anonymized

### Model Training
- **Local Training**: Team-specific configs stay local (not shared across orgs)
- **Opt-out**: Teams can opt out of centralized model improvements
- **Audit Trail**: Track who provided which corrections and when

---

## Summary

The HITL training system transforms Backlog Chef from a **generic AI tool** into **your team's intelligent assistant** that understands:

âœ… Your Definition of Ready
âœ… Your domain-specific risks (Salesforce licenses, GDPR, API limits)
âœ… Your historical patterns (estimation accuracy, common mistakes)
âœ… Your context sources (where to find relevant past work)
âœ… Your team structure (who to route questions to)

**Key Benefits**:
- **Immediate**: Few-shot learning improves next run
- **Adaptive**: Learns your patterns over weeks
- **Transparent**: Shows what it learned and why
- **Measurable**: Tracks accuracy improvement over time

**Expected Improvement Timeline**:
- Week 1: 60% accuracy (many corrections needed)
- Week 4: 80% accuracy (occasional corrections)
- Week 8: 90% accuracy (rare corrections, high trust)
