# Backlog Chef - Production Implementation

Production-ready AI-powered system for transforming meeting transcripts into high-quality Product Backlog Items.

## Overview

This is the **production implementation** of Backlog Chef, separate from the POC proof-of-concept implementations. It uses the multi-agent AI system with intelligent routing, cost tracking, and per-step model selection.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input         â”‚
â”‚  (Transcript)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Pipeline Orchestrator            â”‚
â”‚                                         â”‚
â”‚  Step 1: Event Detection                â”‚
â”‚    â””â”€â†’ Identify meeting type           â”‚
â”‚                                         â”‚
â”‚  Step 2: Extract Candidates             â”‚
â”‚    â””â”€â†’ Parse PBIs from transcript      â”‚
â”‚                                         â”‚
â”‚  Step 3: Score Confidence               â”‚
â”‚    â””â”€â†’ Evaluate PBI quality            â”‚
â”‚                                         â”‚
â”‚  Step 4: Enrich Context (TODO)          â”‚
â”‚    â””â”€â†’ Add historical context          â”‚
â”‚                                         â”‚
â”‚  Step 5: Check Risks (TODO)             â”‚
â”‚    â””â”€â†’ Identify blockers/dependencies  â”‚
â”‚                                         â”‚
â”‚  Step 6: Generate Proposals (TODO)      â”‚
â”‚    â””â”€â†’ Create questions + answers      â”‚
â”‚                                         â”‚
â”‚  Step 7: Readiness Checker (TODO)       â”‚
â”‚    â””â”€â†’ Definition of Ready evaluation  â”‚
â”‚                                         â”‚
â”‚  Step 8: Final Output (TODO)            â”‚
â”‚    â””â”€â†’ Format for multiple destinationsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Model Router  â”‚
         â”‚  (AI Gateway)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚            â”‚
    â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Anthropicâ”‚  â”‚ OpenAI â”‚  â”‚ Ollama â”‚
â”‚ Claude  â”‚  â”‚  GPT   â”‚  â”‚ Local  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### 1. Install Dependencies

```bash
npm install
```

### 2. Set API Keys

```bash
# Required: At least one provider
export ANTHROPIC_API_KEY="sk-ant-..."    # Claude (recommended)
export OPENAI_API_KEY="sk-proj-..."     # OpenAI (alternative)
export GOOGLE_API_KEY="AIza..."          # Gemini (alternative)

# Optional: Azure OpenAI
export AZURE_OPENAI_API_KEY="..."
export AZURE_OPENAI_ENDPOINT="https://..."
export AZURE_OPENAI_DEPLOYMENT="gpt-4o"

# Optional: Local models (no API key needed)
ollama serve  # Start Ollama
ollama pull llama3.2
```

### 3. Run Pipeline

```bash
# Using example transcript
npm start

# Using your own transcript
npm start path/to/your/transcript.txt
```

### 4. View Results

Output is saved to `output/pipeline-output-[timestamp].json`

## Configuration

### Router Configuration

Create or edit `config/model-config.yaml`:

```yaml
# Default model for all steps
defaults:
  provider: anthropic
  model: claude-3-5-haiku-20241022
  currency: EUR

# Fallback strategy
fallback:
  enabled: true
  strategy: cascade
  providers:
    - { provider: anthropic, model: claude-3-5-haiku-20241022 }
    - { provider: openai, model: gpt-4o-mini }
    - { provider: ollama, model: llama3.2:latest }

# Per-step overrides
steps:
  detect_event_type:
    provider: anthropic
    model: claude-3-5-haiku-20241022
    reason: "Fast classification"

  score_confidence:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    reason: "Complex analysis requires more powerful model"

# Cost limits
cost_management:
  per_run_limit_usd: 1.00
  alert_threshold_usd: 0.50
```

## Project Structure

```
src/
â”œâ”€â”€ index.ts                    # Main entry point
â”œâ”€â”€ ai/                         # AI Provider System
â”‚   â”œâ”€â”€ providers/              # Provider implementations
â”‚   â”‚   â”œâ”€â”€ base.ts            # Core interfaces
â”‚   â”‚   â”œâ”€â”€ base-provider.ts   # Base classes
â”‚   â”‚   â”œâ”€â”€ anthropic.ts       # Claude provider
â”‚   â”‚   â”œâ”€â”€ openai.ts          # OpenAI provider
â”‚   â”‚   â”œâ”€â”€ azure-openai.ts    # Azure provider
â”‚   â”‚   â”œâ”€â”€ gemini.ts          # Gemini provider
â”‚   â”‚   â””â”€â”€ ollama.ts          # Local provider
â”‚   â”œâ”€â”€ router/                 # Model Router
â”‚   â”‚   â”œâ”€â”€ model-router.ts    # Routing logic
â”‚   â”‚   â”œâ”€â”€ cost-tracker.ts    # Cost management
â”‚   â”‚   â””â”€â”€ README.md          # Router documentation
â”‚   â”œâ”€â”€ config/                 # Configuration
â”‚   â”‚   â”œâ”€â”€ config-loader.ts   # YAML loader
â”‚   â”‚   â””â”€â”€ provider-registry.ts # Provider factory
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â””â”€â”€ currency-converter.ts
â”œâ”€â”€ pipeline/                   # Processing Pipeline
â”‚   â”œâ”€â”€ orchestrator/           # Pipeline coordinator
â”‚   â”‚   â””â”€â”€ pipeline-orchestrator.ts
â”‚   â”œâ”€â”€ steps/                  # Individual steps
â”‚   â”‚   â”œâ”€â”€ base-step.ts       # Base step class
â”‚   â”‚   â”œâ”€â”€ step1-event-detection.ts
â”‚   â”‚   â”œâ”€â”€ step2-extract-candidates.ts
â”‚   â”‚   â””â”€â”€ step3-score-confidence.ts
â”‚   â””â”€â”€ types/                  # Type definitions
â”‚       â””â”€â”€ pipeline-types.ts
â””â”€â”€ utils/                      # Shared utilities
    â””â”€â”€ logger.ts
```

## Implemented Steps

### âœ… Step 1: Event Detection
- **Purpose**: Identify meeting type (refinement, planning, retrospective, etc.)
- **Model**: Claude 3.5 Haiku (fast, cheap)
- **Output**: Event type with confidence score

### âœ… Step 2: Extract Candidates
- **Purpose**: Parse transcript to extract PBIs
- **Model**: Claude 3.5 Haiku (efficient extraction)
- **Output**: List of candidate PBIs with descriptions

### âœ… Step 3: Score Confidence
- **Purpose**: Evaluate PBI quality and completeness
- **Model**: Claude 3.5 Sonnet (complex analysis)
- **Output**: Quality scores across multiple dimensions

### ğŸš§ Steps 4-8 (TODO)
- Step 4: Enrich with Context
- Step 5: Check Risks & Conflicts
- Step 6: Generate Questions + Proposals
- Step 7: Run Readiness Checker
- Step 8: Final Output (multi-format)

## Programmatic Usage

```typescript
import {
  PipelineOrchestrator,
  createProviderRegistry,
  loadRouterConfig,
  ModelRouter,
} from './src';

async function processMeeting(transcript: string) {
  // Initialize
  const registry = createProviderRegistry();
  const config = loadRouterConfig('./config/model-config.yaml');
  const router = new ModelRouter(registry.getAll(), config);
  const orchestrator = new PipelineOrchestrator(router);

  // Execute
  const output = await orchestrator.execute({
    transcript,
    metadata: {
      meeting_date: '2025-01-19',
      source: 'Zoom Recording',
    },
  });

  // Access results
  console.log(`Found ${output.metadata.total_pbis} PBIs`);
  console.log(`Cost: $${output.metadata.total_cost_usd.toFixed(4)}`);

  for (const pbi of output.pbis) {
    console.log(`- ${pbi.pbi.id}: ${pbi.pbi.title}`);
    console.log(`  Score: ${pbi.scores.overall_score}/100`);
  }

  return output;
}
```

## Pipeline Options

```typescript
await orchestrator.execute(input, {
  // Skip specific steps
  steps: {
    skip: ['enrich_with_context'],
  },

  // Or only run specific steps
  steps: {
    only: ['detect_event_type', 'extract_candidates'],
  },

  // AI configuration
  ai: {
    temperature: 0.7,
    maxTokens: 4096,
  },

  // Output configuration
  output: {
    formats: ['markdown', 'devops'],
    directory: './custom-output',
  },

  // Cost limits
  costLimits: {
    per_run_limit_usd: 2.0,
    alert_threshold_usd: 1.0,
  },
});
```

## Cost Management

The system tracks costs in real-time:

```typescript
// Get cost statistics
const stats = orchestrator.getRouter().getCostStatistics();
console.log(`Total: $${stats.total_cost_usd.toFixed(6)}`);
console.log(`Avg per request: $${stats.average_cost_per_request.toFixed(6)}`);

// Cost by provider
for (const [provider, cost] of Object.entries(stats.cost_by_provider)) {
  console.log(`${provider}: $${cost.toFixed(6)}`);
}
```

Estimated costs per PBI (with default config):
- **Step 1-2** (Haiku): ~$0.002-0.005
- **Step 3** (Sonnet): ~$0.01-0.02
- **Total (Steps 1-3)**: ~$0.015-0.025
- **Full Pipeline (Steps 1-8)**: ~$0.05-0.10 (estimated)

## Multi-Currency Support

All costs are tracked in USD but can be displayed in EUR or GBP:

```yaml
defaults:
  currency: EUR  # or USD, GBP
```

Exchange rates (as of January 2025):
- 1 USD â‰ˆ 0.92 EUR
- 1 USD â‰ˆ 0.79 GBP

## Error Handling

The pipeline provides detailed error information:

```typescript
try {
  const output = await orchestrator.execute(input);
} catch (error) {
  if (error.message.includes('Cost limit')) {
    console.error('Budget exceeded! Consider:');
    console.error('1. Using cheaper models (Haiku instead of Sonnet)');
    console.error('2. Enabling local fallback (Ollama)');
    console.error('3. Increasing cost limits');
  } else if (error.message.includes('Provider unavailable')) {
    console.error('AI provider is down. Check fallback configuration.');
  }
}
```

## Development

### Adding New Steps

1. Create step class extending `BaseStep`:

```typescript
import { BaseStep } from './base-step';

export class MyNewStep extends BaseStep {
  readonly name = 'my_new_step';
  readonly description = 'What this step does';

  protected async executeStep(context, router) {
    // Your step logic here
    const response = await this.makeAIRequest(
      router,
      this.name,
      systemPrompt,
      userPrompt,
      context
    );

    // Update context
    context.myResults = this.parseJSONResponse(response, 'My Step');
    return context;
  }

  canExecute(context) {
    return !!context.previousStepResults;
  }
}
```

2. Add to orchestrator:

```typescript
// In pipeline-orchestrator.ts
private initializeSteps(): PipelineStep[] {
  return [
    // ... existing steps
    new MyNewStep(),
  ];
}
```

3. Add step configuration:

```yaml
# In model-config.yaml
steps:
  my_new_step:
    provider: anthropic
    model: claude-3-5-haiku-20241022
    reason: "Why this model"
```

## Testing

```bash
# Run with example transcript
npm start

# Run with custom transcript
npm start examples/my-transcript.txt

# Check output
cat output/pipeline-output-*.json | jq .
```

## Differences from POC

| Aspect | POC (`poc-step*`) | Production (`src/`) |
|--------|-------------------|---------------------|
| **Purpose** | Learning & validation | Production use |
| **AI System** | Hardcoded Claude API | Multi-provider router |
| **Configuration** | Inline code | YAML configuration |
| **Cost Tracking** | Basic | Comprehensive |
| **Fallback** | None | Automatic |
| **Error Handling** | Basic | Comprehensive |
| **Extensibility** | Limited | Highly modular |
| **Testing** | Manual | Integration tests |

## Next Steps

1. **Complete remaining steps** (4-8)
2. **Add integration tests**
3. **Implement output formatters** (Markdown, DevOps, Confluence)
4. **Add CLI interface** with better UX
5. **Performance optimization** for large transcripts
6. **Add caching** for repeated requests

## Troubleshooting

### "No AI providers initialized"

Set at least one API key:
```bash
export ANTHROPIC_API_KEY="sk-ant-..."
```

### "Cost limit exceeded"

Increase limits in config:
```yaml
cost_management:
  per_run_limit_usd: 2.00
```

### "Failed to parse JSON"

The system has 4-tier fallback parsing. If it still fails, check AI response in logs.

## Documentation

- **AI Router**: See `src/ai/router/README.md`
- **Providers**: See `src/ai/README.md`
- **Architecture**: See `docs/architecture/pact-phases/01-architect-multi-agent-system.md`
- **POC Reference**: See `poc-step*/README.md` (isolated from production)

## License

Proprietary - Backlog Chef
